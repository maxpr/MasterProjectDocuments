
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{mathtools, bm}
\usepackage{amssymb, bm}
\usepackage{graphicx}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in


\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle} % Top center header
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment


% Header and footer for when a page split occurs between problem environments
   
%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------
\newcommand{\logoepfl}{
  \begin{center}
    \includegraphics[width=4cm]{img/epfl.jpg}
  \end{center}
  \vspace{0.3cm}
  \hrule
}


\newcommand{\hmwkTitle}{Decentralized Data Sharing System based on Secure Multiparty Computation} % Assignment title
\newcommand{\hmwkDueDate}{Autumn 2017} % Due date
\newcommand{\hmwkClass}{IN, LCA1} % Course/class
\newcommand{\hmwkClassTime}{} % Class/lecture time
\newcommand{\hmwkClassInstructor}{D.Froelicher, J.Troncoso-Pastoriza} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{Max Premi} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
\title{
\logoepfl
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
\author{\textbf{\hmwkAuthorName}}
\vspace{3in}
}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

\newpage
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}
Unlynx and Prio are two privacy-preserving data sharing systems, with each it's way to encode, decode and aggregate datas. While Unlynx uses homomorphic encryption based on Elliptic Curve ElGamal and zero knowledge proofs, Prio uses Secret-sharing encoding and \textit{secret-shared non-interactive proofs}(SNIP's) to validate the data, which is supposed to perform much better than classic zero knowlege proof in term of computation time, by doing a trade-off execution time/bandwidth.\\
We consider $m$ servers that consitute the collective authority whose goal is to verifiably compute aggregation functions over data send by $n$ data providers.\\
Several problems arise when you want to compare Unlynx and Prio. First, server are static in the first one but not in the second, and the threat model are not exactly the same, as you need to trust at least one server in Unlynx, but all in Prio for correctness. Privacy is assured if at least one server is trusted for both sytem. Then for Unlynx, data providers have all their data encrypted, while Prio do not use homomorphic encryption, and do not need to store data at all. Indeed Unlynx requires the data to be stored encrypted with a collective key, whereas Prio can proceed data decomposed in shares, sent by clients directly.\\
Prio also extends classic private aggregation techniques to enable collection of different class of statistics such as least-square regression, unlike Unlynx System, that can only compute sum and count query over data.\\
The goal of this project is to, first implement Prio in the Unlynx framework, second to implement input validation for Unlynx, and then modify both protocol to be ran with the least significant difference in term of assumption and model.\\
Eventually, if possible, we want to design a system that implement the best of both privacy-preserving data sharing protocols. The idea would be to use Prio encoding to do more than just summing and counting in Unlynx, by removing the encryption at the data provider, and doing the computation locally and then transfer to server in an encrypted way.\\

-------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\newpage
\tableofcontents
\newpage


%----------------------------------------------------------------------------------------
%	BEGIN OF REPORT
%----------------------------------------------------------------------------------------

\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}
Nowadays, tons of data are generated around us about what we do, and are used to compute statistics, by different parties. Even if these statistics are colleted with the goal of learning usefull aggregate informations about the users/population, for example health or work statistics for a country \cite{swiss}, it might end in collecting and storing private data from data provider, which poses a serious privacy and security problem.\\
We can illustrate this example with the numerous problem of Cloud leaking that happened several times in the past years. \cite{pentagon}, or the divulagation of sensitive health data such as suceptibility to contract diseases, that can be used against one individual \cite{sell}. They might even be discloded, sold for profit \cite{sold} or be used by agencies for targeting and mass surveillance goals, as some countries have not highly regulated Data privacy law (we can take the example of the U.S \cite{law}).\\
The need to collect data and to share them in a privacy-preserving way has become crucial in this context, and lot of research has been done on this topic.\\
A lot of techniques have been developed through the years, by major technology companies such as Apple \cite{apple}, but also researcher in Universities \cite{unlynx,prio}.\\
First, sytems that use "Randomize response for differential privacy" \cite{randomized} were developped. It changes a value with probability $p < 0.5$, and by summing a large number of noisy value, the aggregation is still a good estimation of the real values. This technique is well scalable and perform nicely, but assure only \textit{weak privacy}.\\
So encryption system were developped to solve this problem.
However, by gaining \textit{privacy}, these protocols sacrifice \textit{robustness} and \textit{scalability}, which are two important aspect to keep in mind while designing a decentralized system. Privacy is necessary so that no leak in the sensitive data happens, and robustness characterise the correctness of the computation. The trade-off between both should be reasonable, as we do not want any data to be leaked, and the server has to correctly compute under given circunstunces.\\
This difficulties lead to the use of legal agreements rather than technical solutions, as only a few of the systems have been deployed in the real-world. This agreement is not a robust solution for several reasons.\\
One of them is centralization. Centralized system are still widely used \cite{dyadic,centralized} because they are way simpler and use a trusted third party. But these trusted parties still are a single point of failure in the system.\\
Another reason is that data provider have begun to realize the importance and value of their own data.
This is why decentralized sytems are becoming more popular, and desirable. We can also illustrate the growth of decentralized systems with the rise of Cryptocurrencies such as Bitcoin \cite{bitcoin}.\\
In this paper, we present the implementation of Prio into Unlynx, an implementation of Unlynx's input validation and a theoritical comparison and discussion for future possible new sytems that combines, if possible, the best part of each paper.
	
\section*{Contributions}
\addcontentsline{toc}{section}{Contributions}
In this paper, the following contributions are made:\\
- An implementation of Prio SNIPs system in Unlynx, represented as two new protocols, and a new service. It includes the validation and the aggregation. It also contains the different data types supported by Prio.\\

- The implementation of a proof for input range validation for Elliptic Curve ElGamal, using biparing on a specific Elliptic Curve. This allows the server to exclude faulty data sent by possible malicious clients. The range checked is $[0,u^l]$ with $u,l$ taking arbitrary values. \\

- An evaluation of both this protocol in terms of privacy and efficiency, with a comparison with the most similar settings. Then a conclusion on which system suits better the the needs of large scale real world scenario.


\section*{Background}
\addcontentsline{toc}{section}{Background}
\subsection*{Collective Authority}
Nowadays, applications and systems rely on third party authorities to provide security service. For example the creation of certificate to prove ownership of a public key. A collective authority is a set of server that are deployed in a decentralized and distributed way, to support a given number of protocols.\\
Each of them possesses a private-public key pair $(k_i,K_i)$, where $K_i = k_i B$ with $k_i$ is a scalar and $K_i$ a point in a given Elliptic Curve. This authority construct a public key $K = \sum_{i=1}^{m}{K_m}$ which is the sum of all the server's public key. So to decrypt a message, each server partially decrypt a message encrypted using $K$, thus the collective authority key provides strongest link security, as no intermediate can decrypt the data without the contribution of all the servers.

\subsection*{ElGamal}
All scalar are picked in a field $\mathbb{Z}_p$.\\
For Unlynx, data are encrypted by using Elliptic Curve ElGamal, more precisely, $P$ is a public key, $x$ is a message mapped to a point and $B$ is a bassepoint on the curve $\gamma$. The encryption is the following, with $r$ a random nonce:\\
$E_P(x) = (rB,x+rP)$. The homomorphic properties states that $\alpha E_P(x_1) + \beta E_P(x_2) = E_P(\alpha x_1+ \beta x_2)$\\
To decrypt, the owner of the private key $P = pB$ multiplies $rB$ and $p$ to get $rP$ and substract from $x + rP$.\\

\subsection*{Arithmetic Circuits}
An arithmetic circuit $C$ over a finite field $\mathbb{F}$ takes as input a vector $x = (x^{(1)},...x^{(L)}) \in \mathbb{F}^L $. It is represented as an acyclic graph with each vertex either be an \textit{input}, \textit{output} or a \textit{gate}.\\
There are only two types of gates, addition and multiplication ones, all in finite field $\mathbb{F}$.\\
A citcuit $C$ is just a mapping $\mathbb{F}^L \rightarrow \mathbb{F}$, as evaluatig is a walk thourght the circuit from inputs to outputs.

\subsection*{Beaver's MPC}
A Beaver triple is defined as follow:\\
$(a,b,c) \in \mathbb{F}^3$, chosen at random with the constraint that $a \cdot b = c$.\\
As we use it in a multiparty computation context, each server $i$ holds a share $([a]_i , [b]_i , [c]_i) \in \mathbb{F}^3$.\\
Using these shares, the goal is to multiply two number $x$ and $y$ without leaking anything about them. In Prio the goal is to multiply shares $[x]_i$ and $[y]_i$.\\
To do so the following values are computed:
$$[d]_i = [x]_i -[a]_i  \quad  ;  \quad  [e]_i = [y]_i - [b]_i$$ 
Then from this shares, each server can compute $d$ and $e$ and compute this formula:\\
$$\sigma_i = de/m + d[b]_i +e[a]_i + [c]_i$$
The sum of these shares yields:

\begin{equation}
\begin{split}
& \sum_{i} \sigma_i = \sum_{i}{(de/m +d[b]_i + e[a]_i + [c]_i)}\\
 & = de +db +ei + c\\
 & = (x-a)(y-b)+ (y-a)b + (z-b)a + c\\
 & = (x-a)y + (y-b)a + c\\
 & = xy -ab + c\\
 & = xy\\
\end{split}
\end{equation}
As $\sum_{i} \sigma_i = xy$, it implies that $\sigma_i = [xy]_i$ 

\subsection*{Affine-aggregatable encodings (AFEs) functions}
Given the fact that each data provider $i$ holds a value $x_i \in D$, and the server have an aggregation function $f : D^n \rightarrow A$, whose range is a set of aggregates $A$, an AFE gives an efficient way to encode data such that it is possible to compute the value of the aggregation function $f(x_1,...,x_n)$ given only the \textit{sum of the encodings} $x_1,....,x_n$.\\
It consist of three Algorithm (Encode, Valid, Decode) defined in a field $\mathbb{F}$ and two integers $k' \leq k$\\
\begin{itemize}
\item \textbf{Encode($x$)}: maps an input $x \in D$ to its encoding in $\mathbb{F}^k$
\item \textbf{Valid($y$)}: returns true if and only if $y$ is a valid encoding of some item in $D$
\item \textbf{Decode($\sigma$)}: $\sigma = \sum_{i=1}^{n}{Trunc_{k'}(Encode(x_i))} \in \mathbb{F}^{k'}$ is the input (Trunc takes the $k' \leq k$ components of the encoding), and it outputs the aggregation result $f(x_1,...,x_n)$.
\end{itemize}

\subsection*{Bilinear parings over Elliptic Curve}
Let $G_1$ and $G_T$ be additive group of points of an elliptic curve $G$ over a field $\mathbb{F}$ of order $n$ and with identity $O$. Then the mapping $e: G_1 \times G_1 \rightarrow G_T$ satisfies the following conditions:\\
For all $R,S \in G_1$ and $x \in \mathbb{F}$, $e(R,S)(x) = e(Rx,S) = e(R,Sx)$\\
For $B$ the base point, $e(B,B) \neq O$, and the mapping is efficiently computable.


\section{Unlynx System}
\subsection{Model}
Unlynx \cite{unlynx} is a privacy-preserving data sharing system developed by LCA1 \cite{lca} in collaboration with DeDiS \cite{dedis}.\\
It consists of a collective authority (CA) formed by a number $m$ of server $S_1,...,S_m$, and $n$ data providers $DP_1,...DP_n$ containing sensitive data, encrypted using EC ElGamal, following the key scheme describe in the \textbf{Background} Section. These DPs combined represent a distributed database that is used to awnser queries made by a querier $Q$. The querier and DPs choose one server of the CA to communicate with and can change this choice at any given time.\\
\textbf{Functionality}: Unlynx should permit SQL queries of the form SELECT SUM(*)/COUNT(*) FROM DP,.... WHERE * AND/OR GROUP BY *, with any number of * clauses.\\
\textbf{Privacy and Robustness}: Both are assured if at least one server is trusted, as we use the fact that we are allowed to publish ciphertext and their aggregation to show that the computation at the server are actually correct. It leaks nothing as all data are encrypted.

\subsection{Threats}
\textbf{Collective authority servers} It is assumed an Anytrust model \cite{anytrust}. It does not requires any particular server to be trusted or to be honest-but-curious. The moment it exists one server that is not malicious, functionality, security and privacy are guaranteed.\\
\textbf{Data providers} are assumed to be honest-but-curious. The system does not protect against malicious DPs sending false infomations, but a solution will be discuss  in Section the \textbf{Input range validation for Elliptic Curve ElGamal}.\\
\textbf{Queriers} are assumed to be malicious, and can collude between themselves or with a subset of the CA servers.\\
It is also assumed tha all network communication is encrypted and authenticated, by using a protocol like TLS for example.

\subsection{Pipeline and proof}
The protocol start when a querier wants to retrieve some information about sensitive data. It sends the query to one of the server of the CA. Upon receiving, the server broadcast this query to the other servers in the collective authority.\\
From here the data are privately and securely processed by the CA, before sending back the result to the querier, encrypter over the public key of the Querrier. During all the steps of the protocol, the server will never get the data in clear.\\
The pipeline is the following: Encryption, Verification Shuffle, Distributed Deterministig Tag, Collective Aggregation, Distributed Results Obfuscation, Key Switch. At the end of this pipeline, the querier get the data and can decrypt them to get the aggregate statistics he asked for, without any server seeing the data in clear, or knowing from which data provider the data are from.
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{img/unlynxPip.png}
\caption{Unlynx query processing pipeline}
\end{figure}
\\
\textbf{Proofs} are done thanks to zero-knowledge systems, to preserve privacy. There is one for each state of the pipeline, to illustrate a zero knowledge proof, while doing the aggregation phase it publishes the ciphertexts, and the result of their aggregation. As the data is encrypted using Elliptic curve ElGamal, it leaks nothing about the data alone. One of the problem in Unlynx is that it exists no Input range proof for data coming from the data providers, meaning the DPs can send fraudulous data (by giving a very big number or really small) and as data are encrypted we have no way to directly check this. This make the whole result and computation obsolete, and this is why in the basic Threat model, data providers are assumed honest-but-curious. This input validation proof is implemented and described in the following lines.\\

\subsection{Input range validation for Elliptic Curve ElGamal}

A problem with the encryption of data is that we can not be sure if the data sent are correct. An example is if we want to aggregate a field were values should be in range $[0,100)$, in the current system if a malicious data provider want to send a value of $15000$ to falsify information, it can do it. This section present an  interactive algorithm based on a classic ElGamal input range validation \cite{range}, and we will discuss its non-interactive form in the section dedicated to implementation.\\
We define the scalar in the elliptic curve over the field $\mathbb{Z}_p$. Also we call $e()$ the billinear mapping that is describe in the Background section.\\
This is the algorithm that allows to check the validity of a secret $\sigma$ in a range $ [0,u^l]$. The algortihm can be adapted to check any range $[a,b]$.\\

\begin{algorithm}
\caption{Interactive Range Validation}\label{euclid}
\begin{algorithmic}[1]

\State For the algorithm scalability the terms \textit{Prover} and \textit{Verifier} are used. In our case it should be clear that the prover is the data provider and the verifier the CA containing all the servers.\\
\State \textbf{Common Input:} $B, P, u, l$ and commitment $C$
\State \textbf{Prover Input:} 
$\sigma, r$ such that $C = \sigma + Pr$, with $\sigma \in [0,u^l)0$\\
\State  \textbf{P $\leftarrow$ V} verifiers pick $x_i \in \mathbb{Z}_p$ define $y_i \gets Bx_i$\\
send $A_{i,j} \gets B(x_i+j)^{-1}$ $ \forall j \in \mathbb{Z}_u$ and $ i \in \{0,...,m\}$\\
\State \textbf{P $\rightarrow$ V} Then prover encode the signature of the value to check in base $u$ with randomly picked $v_j$.\\
So $\forall j \in \mathbb{Z}_l $ such that $ \sigma = \sum_{j}{\sigma_j u^j}$, it picks $v_j \in \mathbb{Z}_p$ and sends $V_{i,j} = A_{i,\sigma_j}v_j$ back to server $i$\\
\State \textbf{P $\rightarrow$ V} prover pick 3 values $s_j,t_j,m_j \in \mathbb{Z}_p $, $\forall j \in \mathbb{Z}_l$ and sends:\\
$a_{i,j} \gets e(V_{i,j},B)(-s_j)+e(B,B)(t_j)$\\
$D \gets \sum_{j}{(u^j s_j + Pm_j)}$\\
\State \textbf{P $\leftarrow$ V} Verifier sends a random challenge $c \in \mathbb{Z}_p$\\
\State \textbf{P $\rightarrow$ V} Prover sends the following value for Verifiers to compute verification.\\
$ \forall j \in \mathbb{Z}_l$, $Z_{\sigma_j} \gets s_j-\sigma_j c$ and $Z_{v_j} \gets t_j-v_j c$\\
$Z_r = m -rc $, where $m = \sum_{j}{m_j}$\\
\State \textbf{Verifier} $i$ check that $D = Cc + PZ_r + \sum_{j}{(Bu^j Z_{\sigma_j}}) $\\
$a_{i,j} = e(V_{i,j},y)c + e(V_{i,j},B)(-Z_{\sigma_j}) + e(B,B)(Z_{v_j})$, $\forall j \in \mathbb{Z}_l$
\end{algorithmic}
\end{algorithm}

This algorithm as been adapted from the original one \cite{range}, used on classic ElGamal encryption. The soundness follows from the unforgeability of the Boneh-Boyen signature. The prover need to compute $5l$ point multiplication in the protocol.\\
\textbf{Arbitrarty Range}\\
To handle an arbitrary range $[a,b]$, it is needed to show that $\sigma \in [a,a+u^l]$ AND $ \sigma \in [b-u^l,b]$, this leads to the following formula:\\
\begin{gather*}
 \sigma \in [b-u^l,b] \Longleftrightarrow   \sigma - b + u^l \in [0,u^l) \\
\sigma \in [a,a+u^l] \Longleftrightarrow   \sigma - a  \in [0,u^l)\\
\end{gather*}
The only modification necessary in the algortihm is the verifier's check which is now:
\begin{gather*}
D = Cc+B(-B+u^l)+P(Z_r)+\sum_j{B(Z_{\sigma_j})}\\
D= Cc+B(-A)+P(Z_r)+\sum_j{B(Z_{\sigma_j})}
\end{gather*}

\section{Prio System}
\subsection{Model}
Prio \cite{prio}  is also a privacy-preserving data sharing system developed Standford University.\\
It consists of a collective authority (CA) formed by a number $m$ of server $S_1,...S_m$,and each data provider holds a private value $x_i$ that is sensitive. 
Unlike Unlynx, Prio does not encrypt private value $x_i$ that why it is a more challenging aggregation in terms of privacy.\\
The basics of Prio is that each data $x$ is splitted in $m$ shares such that $\sum^m_{k=1}{x_k} = x $ in a defined a finite field $\mathbb{F}$ ie, modulo a prime $p$. This encoding helps keeping privacy, as getting $m-1$ shares doesn't leak anything about $x$ in itself.\\
Communication is assumed to be done in secure channels as previously described.\\
\textbf{Functionality}: Prio should permit the collective authority to compute some aggregation function $f(x_1,...x_n)$ over private values of data providers, in a way that leaks as little as possible about these, except what can be inferred from the aggregation itself.\\
It is also possible to gather more complex statistcs such as variance, standard deviation, frequency count or even sets intersections/unions. All of the function quoted before are only computed from an encoding called AFE
\textbf{Privacy and Robustness}: Privacy is assured if at least one server is trusted, but robustness is satisfied if and only if all server are trusted, as you cannot be assured that computation at the server are correct.

\subsection{Threats}
\textbf{Collective authority servers} In Prio, it is needed that one server is not malicious and trusted, so that security and privacy are guaranteed.\\
Robustness against malicious server seems desirable but doing so would cost privacy and perfomance degradation, which is not wanted.\\
\textbf{Data providers} are assumed to be malicious. The system protects itself against malicious DPs. All data that does not pass the SNIPs proof, will be discarded.\\

\subsection{Pipeline and proof}
Pipeline is a little different than Unlynx, as first the proof is run on the data when they arrived, if it passes the proof, it is stored and aggregate later with more data.\\
First, it is needed to define an arithmetic circuit for each data provider, $Valid(.)$.
When the data provider run the circuit with its secret value as input, it output $1$, i.e $Valid(x_i)=1$.\\
This circuit is only defined in function of the number of bit of each shares $[x_i]_j$ from the data provider, so it will also send a configuration file to the server so that it an reconstruct the circuit to verify the input too. From this circuit, $3$ polynomials are extracted $f,g $ and $h$.\\

The data providers first upload their shares $[x_i]_1,...,[x_i]_m$ of private value and a share of polynomial $h$ extracted from arithmetic circuit.\\
The servers verify the SNIPs provided by data providers to jointly confirm that $Valid(x_i) = 1$. If it fails, the server discard the submission.\\
Then each servers saved in an accumulator the data they need to aggregate and run the collective aggregation over the verifed datas.\\
When received enough input, they each publish their aggregation result to yield the final aggregation ( which is the sum of all aggregation ) result.

\subsection{Prio SNIPs}
In this section, the SNIP protocol will be detailled a litlle more.\\
\textit{Assumption}: The \textbf{Valid} circuit have $M$ multiplication gates, we work over a field $\mathbb{F}$ such that $ 2M << |\mathbb{F}| $\\
\subsubsection{DP evaluation}
First the DP evaluates the circuit \textbf{Valid} on its input $x$. It construct three polynomials $f,g $ and $h$ which encode respectively the inputs wire and the ouput wire of each of the $M$ multiplication gates in the \textbf{Valid($x$)} circuit.\\
This step is done by polynomial interpolation to construct $f,g$ and get $h$ by multiplying those.\\
So polynomials $f,g$ have a degree at most $M-1$ while $h = f \cdot g$ have a degree at most $2M-2$.\\
Then the DP splits the polynomial $h$ using additive secre sharing and send the $i$th ($[h]_i$) share to server $i$
\subsubsection{Consistency checking at the server}
At this time each server $i$ holds a share $[x]_i$ and $[h]_i$ send by the data provider. From both this values, the servers can reproduce $[f]_i$ and $[g]_i$ without communicating with each others.\\
Indeed $[x]_i$ is a share of the input, and $[h]_i$ contains a share of each wire value coming out of a multiplication gate. Thus, it can derive all other values via affine operation on the wire.
\subsubsection{Polynomial identity test}
Now each server has reconstructed shares $[\hat{f}]_i, [\hat{g}]_i$ from $[h]_i$ and $[x]_i$. It holds that $ \hat{f} \cdot \hat{g} = h$ if and only if the servers collectively hold a set of wire value that, when summed is equal to the internal wire value of the \textbf{Valid($x$)} circuit computation.\\
All exeute the Schwartz-Zippel randomized polynomial identity test [REF] to check if relation holds and no data have been corrupted or malicious DPs have tried to send wrong data.\\
The principle is that if $ \hat{f} \cdot \hat{g} \neq h$, then the polinomial $ \hat{f} \cdot \hat{g} - h$ is a non-zero polynomial of degree at most $2M-2$ zeros in $\mathbb{F}$. It can have at most $2M-2$ zeros, so we choose a random $r \in \mathbb{F}$ and evaluate this polynomial, the servers will detect with probability at least $1 - \frac{2M-2}{|F|}$ that $ \hat{f} \cdot \hat{g} \neq h$.\\
The servers can use a linear operation to get share $\sigma_i = [ \hat{f}(r) \cdot \hat{g}(r) - h(r)]_i$. Then publish to ensure that $\sum_{i}{\sigma_i} = 0 \in \mathbb{F}$. If it is not $0$ the servers reject the client submission.
\subsubsection{Multiplication of shares}
Finally all servers need to multiply the shares $ [\hat{f}(r)]_i $ and $ [\hat{g}(r)]_i$ to get the share $[\hat{f}(r)]_i \cdot [\hat{g}(r)]_i$ without leaking anything to each other about the values of the two polynomials. It is here that the Beaver MPC enter the computation. Each servers also received from a trusted dealer a one-time-use shares $([a]_i,[b]_i,[c]_i) \in \mathbb{F}^3 $ such that $a \cdot b = c \in \mathbb{F}$. We can from this share, efficiently compute a multiparty multiplication of a pair secret-shared values. This only require each server to broadcast a single message.\\
The triple is generated by the data provider. The servers will be able to compute correcctly if values are corect. Moreover, even if the data provider send wrong values, the server still catch the cheating client with high probability. Indeed if $a \cdot b \neq c \in \mathbb{F}$ then we can write $a \cdot b = (c + \alpha) \in \mathbb{F}$. We can then run the polynomial identity test with $\hat{f}(r) \cdot \hat{f}(r) - h(r) + \alpha = 0$. The servers will catch a wrong input with probability at least $1 - \frac{2M-2}{|F|}$.

\subsubsection{Output verification}
If all servers are honest, each will hold a set of shares of the values if the \textbf{Valid} circuit. To confirm that \textbf{Valid($x$)} is indeed $1$ they only need to publish their share of the \textit{output wire}. Then, all server sum up to confirm that its indeed equal to $1$, except with some small failure probability due to the polynomail identity test.


\section{Implementation}
 This section will detail what was implemented more throughouly.
First we will abord the implementation of Prio:\\

Prio code \cite{priocode} was implemented in Go, with 3 depedencies in C (FLINT, GMP and MPFR) to execute polynomial operations by Henry Corrigan-Gibbs.\\
This paper contribution is mostly porting code to use the multiparty computation and SNIPs in the Unlynx framework. So most of the code is used directly from the repository, but all the communication server/data provider has been reworked to be compatible with the Tree structure used in Unlynx \cite{unlynxcode}.\\
To follow the structure, the aggregation and verification protocol were splitted in two different protocols, even if they both work together to get the final result.\\
To be more precise, data provider send shares $[x_i]$ and $[h_i]$, servers do the SNIPs proof by evaluating the share on the \textit{Valid()} circuit, do the MPC to fuse each validation of the shares , if it passes it will aggregate the result of the SNIPs, else it will discard.\\
Let's describe the easiest protocol, the aggregation:\\
The protocols are run at each server $j$, and each protocol has received a share $[x_i]_j$ from dataprovider $i$, represented by a \textit{type big.Int} in Go. The protocol structure is a binary Tree.\\

When several data have been received, aggregation start by the root protocol, that notify all children that the aggregation will start, and wait for the children to send their local sum. This notification goes down the Tree until there are no more children, and at this point each leaf $l$ aggregate locally the share by simply summing $\sum_{i}[x_i]_l$, and send the result to their unique parents. On receiving the response from its children, the other nodes aggregates locally their own shares, and send to the parent. This is done until the root as received all the data and aggregate all the data and publish them.\\

However, Prio has a more interesting feature that help aggregating different type of data and doing different aggregation function. Indeed, one can represent the value to aggregate with an AFE, where the AFE will be an array of \textit{big.Int}. This helps computing OR and AND, MIN and MAX and even set intersection/union given only the aggregation of the special AFE encoding for each of this functions.\\
To illustrate an AFE, let's see the OR. To do OR($x_1,...x_n)$ where $x_i \in \{0,1\} $, $x$ is encoded as followed in $\mathbb{F}_2^{\lambda}$ (for a $\lambda$-bit string):
\begin{align*}
    Encode(x)=\left\{
                \begin{array}{ll}
                  \lambda		\text{                                          if x = 0}\\
                  \text{Random element} \in \mathbb{F}^{\lambda}_{2}		\text{											if x = 1}\\
                \end{array}
              \right.
\end{align*}
The Valid algorithm is always true as long as you have same size encoding. To Decode, we output 0 if and only if the $\lambda$-bit string is composed of $\lambda$ 0. This AFE output the boolean OR-private of the values with probability $p = 1- 2^{-\lambda}$, over the randomness of the encoding.



Now we're going to look at the input range validation implementation:

\begin{algorithm}
\caption{Non-Interactive Range Validation}\label{euclid}
\begin{algorithmic}[1]
\State \textbf{Common Input:} $B, P, u, l$ and commitment $C$.
\State \textbf{Prover Input:} $\sigma, r$ such that $C = \sigma + Pr$, $\sigma \in [0,u^l)$\\

\State \textbf{Initialization}: All server $i$ in the collective authority compute the following values :\\
Pick a random $x_i \in \mathbb{Z}_p$\\
$y_i \gets Bx_i$\\
$A_{i,j} \gets B(x_i + j)^{-1} $, $\forall i \in \mathbb{Z}_u$\\
\State  \textbf{Servers} make their signature public as well as the key $y_i$. When a query is issued by a querier $Q$, we now assume that the range are contained in the query and broadcasted by the server as usual to the data provider.\\

\State \textbf{Then Data provider} encodes the signature of the value to check in base $u$ with randomly picked $v_j$.\\
So $\forall j \in \mathbb{Z}_l $ such that$ \sigma = \sum_{j}{\sigma_j u^j}$, it picks $v_j \in \mathbb{Z}_p$ and compute $V_{i,j} = A_{i,\sigma_j}v_j$\\
It also picks 3 values $s_j,t_j,m_j \in \mathbb{Z}_p $, $\forall j \in \mathbb{Z}_l$ and sends:\\
First : a value $c = H(B,C,y_i)$, where $H()$ is a cryptographic hash function.\\  Then: $Z_r = m-rc$ and $D \gets \sum_{j}{(u^j s_j + Pm_j)}$\\
And eventually $ \forall j \in \mathbb{Z}_l$\\
$a_{i,j} \gets e(V_{i,j},B)(-s_j)+e(B,B)(t_j)$\\ $Z_{\sigma_j} \gets s_j-\sigma_j c$ and $Z_{v_j} \gets t_j-v_j c$\\
To be more precise the data provider sends the following values to ALL servers: $ c, Z_r, Z_{v_j}, Z_{\sigma_j}$ with $C$ the encrypted value public, and $D, a_{i,j}$ value published to check proof.\\

\State Server $i$ checks that $D = Cc + PZ_r + \sum_{j}{(u^j Z_{\sigma_j}}) $\\
$a_{i,j} = e(V_{i,j},y)c + e(V_{i,j},B)(-Z_{\sigma_j}) + e(B,B)(Z_{v_j})$, $\forall j \in \mathbb{Z}_l$ , and publish the result.\\

Then the server responsible for the data provider keep the value if all the published value match the one computed by the data provider.
\end{algorithmic}
\end{algorithm}

\begin{itemize}

\item what is implemented in a little more detailled
\item optimization apported by code from github and not aborded in details

\end{itemize}


\section{Performance evaluation}

Explain a little more about comparison and test settings


\begin{itemize}


\item Scaling with number of server
\item Scaling with number of client
\item Scaling with fairly high number of both 
\item Time and Bandwidth dilemna

\end{itemize}


\section{Comparison Theory and Performance}
Comparison but explaining why it's cannot be perfectlhy matched, because of the differents assumptions.

\section{Future Work}

\newpage
\section*{Conclusion}
\addcontentsline{toc}{section}{Conclusion}
iufznfiozfoezizefez fze
fezfezfezfezfezfezf
ezfezfezfzefezfezfezfezfezfzefzefezfezfezfezfezfezfezfz

\newpage
\begin{thebibliography}{9}

\bibitem{unlynx} 
David Froelicher, Patricia Egger, João Sa Sousa, Jean Louis Raisaro, Zhicong Huang, Christian Mouchet, Bryan Ford and Jean-Pierre Hubaux.\\
\textit{UnLynx:} A Decentralized System for Privacy-Conscious Data Sharing. 
\textit{EPFL}
 
\bibitem{prio} 
Henry Corrigan-Gibbs and Dan Boneh.\\
\textit{Prio}: Private, Robust, and Scalable Computation of Aggregate Statistics.
\textit{Standford University}


\bibitem{range} 
Jan Camenisch, Rafik Chaabouni, and abhi shelat\\
Efficient Protocols for Set Membership and Range Proofs.
\textit{IBM Research, EPFL, U. of Virginia}

\bibitem{expose}
Keller,J., Lai,K.R., and Pelroth, N\\
How many times has your personal information been exposed to hackers ?\\
\\\texttt{http://www.nytimes.com/interactives/2015/07/29technology/personaltech/what-parts-of-your-information-have-been-exposed-to-hackers-quiz.html}

\bibitem{pentagon}
Classified Pentagon data leaked on the public cloud, BBC news
\\\texttt{http://www.bbc.com/news/technology-42166004}

\bibitem{apple}
Greenberg, A.\\
Apple's 'differential privacy' is about colleccting your data- but not your data.
\\\texttt{https://www.wired.com/2016/06/apples-differential-privacy-collection-data/}

\bibitem{swiss}
Departement federal de l'economie, de la formation et de la recherche DEFR.
\\\texttt{https://www.amstat.ch}

\bibitem{lca}
LCA1 laboratory, EPFL.
\\\texttt{http://lca.epfl.ch/}

\bibitem{dedis}
DeDis laboratory, EPFL.
\\\texttt{https://dedis.epfl.ch/}

\bibitem{randomized}
Warner, S. L.\\
Randomized response: A survey technique for eliminating evasive bias.\\
\textit{Journal of the American Statistical Association 60,309 (1965),63-69}

\bibitem{sold}
Smith, B.\\
Uber executive suggest digging up dirt on journalits.
\\\texttt{http://www.buzzfeed.com/bensmith/uber-executive-suggests-digging-up-dirt-on-journalists}

\bibitem{dyadic}
Dyadic security \texttt{https://www.dyadicsec.com/}

\bibitem{centralized}
Dan Bogdanov, Sven Laur, and Jan Wilemson.\\
\textit{Sharemind}: A framework for fast privacy-preserving computations. In \textit{European Symposium on Research in Computer Security}

\bibitem{anytrust}
David I Wolinsky, Hery Corrigan-Gibbs, Bryan Ford, and Aaron Jonhson.\\
Scalable anonymous group communication in the anytrust model. In \textit{5th European Workshop on System Security. 2012}


\bibitem{priocode}
Prototype implementation of Prio in Go
\\\texttt{https://github.com/henrycg/prio}

\bibitem{unlynxcode}
Decentralized privacy-preserving data sharing tool : Unlynx\\
\texttt{https://github.com/lca1/unlynx}

\bibitem{sell}
The business of Data selling\\
\texttt{https://www.theguardian.com/technology/2017/jan/10/medical-data-multibillion-dollar-business-report-warns}

\bibitem{law}
Law on Data protection in the United-States\\
\texttt{https://content.next.westlaw.com/Document/I02064fbd1cb611e38578f7ccc38dcbee/View/FullText.html?contextData=(sc.Default)\& transitionType=Default\& firstPage=true\& bhcp=1}


\bibitem{bitcoin}
Bitcoin: A Peer-to-Peer Electronic Cash System.\\
\texttt{https://bitcoin.org/bitcoin.pdf}



\end{thebibliography}


\end{document}
